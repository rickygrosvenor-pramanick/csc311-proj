{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train_data.csv\")\n",
    "val_df = pd.read_csv(\"data/valid_data.csv\")\n",
    "student_meta = pd.read_csv(\"data/student_meta.csv\")\n",
    "question_meta = pd.read_csv(\"data/question_meta.csv\")\n",
    "subject_meta = pd.read_csv(\"data/subject_meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542 1774\n"
     ]
    }
   ],
   "source": [
    "n_students = max(student_meta[\"user_id\"]) + 1\n",
    "n_questions = max(question_meta[\"question_id\"]) + 1\n",
    "print(n_students, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -1.],\n",
       "        [ 1., -1.],\n",
       "        [ 0., -1.],\n",
       "        ...,\n",
       "        [ 1., -1.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_meta_tensor = torch.zeros(n_students, 2)\n",
    "user_id = torch.tensor(student_meta['user_id'].values, dtype=torch.int32)\n",
    "gender = torch.tensor(student_meta['gender'].values, dtype=torch.float32)\n",
    "premium_pupil = torch.tensor(\n",
    "\tstudent_meta['premium_pupil'].fillna(-1.0).values, dtype=torch.float32\n",
    ")\n",
    "student_meta_tensor[user_id, 0] = gender\n",
    "student_meta_tensor[user_id, 1] = premium_pupil\n",
    "student_meta_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDataset(Dataset):\n",
    "\tdef __init__(self, df):\n",
    "\t\tself.user_ids = df['user_id'].values\n",
    "\t\tself.question_ids = df['question_id'].values\n",
    "\t\tself.is_correct = df['is_correct'].values\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.is_correct)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.user_ids[idx], self.question_ids[idx], self.is_correct[idx]\n",
    "\n",
    "class StudentQuestionNet(nn.Module):\n",
    "\tdef __init__(self, student_embed_dim, question_embed_dim, student_meta_dim, hidden_layers, dropout_p=0.3):\n",
    "\t\tsuper(StudentQuestionNet, self).__init__()\n",
    "\t\tinput_dim = student_embed_dim + question_embed_dim + student_meta_dim\n",
    "\t\tlayers = []\n",
    "\n",
    "\t\tfor hidden_dim in hidden_layers:\n",
    "\t\t\tlayers.append(nn.Linear(input_dim, hidden_dim))\n",
    "\t\t\tlayers.append(nn.BatchNorm1d(hidden_dim))\n",
    "\t\t\tlayers.append(nn.ReLU())\n",
    "\t\t\tlayers.append(nn.Dropout(dropout_p))\n",
    "\t\t\tinput_dim = hidden_dim\n",
    "\n",
    "\t\tlayers.append(nn.Linear(input_dim, 1))\n",
    "\t\tself.network = nn.Sequential(*layers)\n",
    "\t\n",
    "\tdef forward(self, student_embed, question_embed, student_meta):\n",
    "\t\tcombined = torch.cat([student_embed, question_embed, student_meta], dim=-1)\n",
    "\t\treturn self.network(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataloaders(train_df, val_df, batch_size):\n",
    "\ttrain_dataset = QuestionDataset(train_df)\n",
    "\ttrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\tval_dataset = QuestionDataset(val_df)\n",
    "\tval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\treturn train_dataloader, val_dataloader\n",
    "\n",
    "def initialize_model_and_optimizers(n_students, n_questions, student_embed_dim, question_embed_dim, student_meta_dim,\n",
    "\t\t\t\t\t\t\t\t\thidden_layers, dropout_p, learning_rate, device):\n",
    "\tstudent_embed = nn.Parameter(torch.randn(n_students, student_embed_dim).to(device))\n",
    "\tquestion_embed = nn.Parameter(torch.randn(n_questions, question_embed_dim).to(device))\n",
    "\n",
    "\tmodel = StudentQuestionNet(\n",
    "\t\tstudent_embed_dim=student_embed_dim,\n",
    "\t\tquestion_embed_dim=question_embed_dim,\n",
    "\t\tstudent_meta_dim=student_meta_dim,\n",
    "\t\thidden_layers=hidden_layers,\n",
    "\t\tdropout_p=dropout_p\n",
    "\t).to(device)\n",
    "\n",
    "\tmodel_optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\tembed_optimizer = torch.optim.Adam([student_embed, question_embed], lr=learning_rate)\n",
    "\n",
    "\treturn model, student_embed, question_embed, model_optimizer, embed_optimizer\n",
    "\n",
    "def train_step(model, train_dataloader, student_embed, question_embed, student_meta_tensor, criterion, model_optimizer,\n",
    "\t\t\t   embed_optimizer, device):\n",
    "\tmodel.train()\n",
    "\ttrain_loss = 0.0\n",
    "\tcorrect_predictions = 0\n",
    "\ttotal_samples = 0\n",
    "\n",
    "\tfor user_ids, question_ids, targets in train_dataloader:\n",
    "\t\tuser_ids = user_ids.to(device)\n",
    "\t\tquestion_ids = question_ids.to(device)\n",
    "\t\ttargets = targets.float().unsqueeze(1).to(device)\n",
    "\n",
    "\t\tuser_embeds = student_embed[user_ids]\n",
    "\t\tquestion_embeds = question_embed[question_ids]\n",
    "\t\tstudent_meta = student_meta_tensor.to(device)[user_ids]\n",
    "\n",
    "\t\tlogits = model(user_embeds, question_embeds, student_meta)\n",
    "\t\tloss = criterion(logits, targets)\n",
    "\n",
    "\t\tmodel_optimizer.zero_grad()\n",
    "\t\tembed_optimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\tmodel_optimizer.step()\n",
    "\t\tembed_optimizer.step()\n",
    "\n",
    "\t\ttrain_loss += loss.item()\n",
    "\t\tpredictions = torch.sigmoid(logits) > 0.5\n",
    "\t\tcorrect_predictions += (predictions == targets).sum().item()\n",
    "\t\ttotal_samples += targets.size(0)\n",
    "\n",
    "\treturn train_loss / len(train_dataloader), correct_predictions / total_samples\n",
    "\n",
    "\n",
    "def val_step(model, val_dataloader, student_embed, question_embed, student_meta_tensor, criterion, device):\n",
    "\tmodel.eval()\n",
    "\tval_loss = 0.0\n",
    "\tcorrect_predictions = 0\n",
    "\ttotal_samples = 0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor user_ids, question_ids, targets in val_dataloader:\n",
    "\t\t\tuser_ids = user_ids.to(device)\n",
    "\t\t\tquestion_ids = question_ids.to(device)\n",
    "\t\t\ttargets = targets.float().unsqueeze(1).to(device)\n",
    "\n",
    "\t\t\tuser_embeds = student_embed[user_ids]\n",
    "\t\t\tquestion_embeds = question_embed[question_ids]\n",
    "\t\t\tstudent_meta = student_meta_tensor.to(device)[user_ids]\n",
    "\n",
    "\t\t\tlogits = model(user_embeds, question_embeds, student_meta)\n",
    "\t\t\tloss = criterion(logits, targets)\n",
    "\t\t\tval_loss += loss.item()\n",
    "\n",
    "\t\t\tpredictions = torch.sigmoid(logits) > 0.5\n",
    "\t\t\tcorrect_predictions += (predictions == targets).sum().item()\n",
    "\t\t\ttotal_samples += targets.size(0)\n",
    "\n",
    "\treturn val_loss / len(val_dataloader), correct_predictions / total_samples\n",
    "\n",
    "def checkpoint(epoch, experiment_path, avg_val_loss, val_accuracy, model, student_embed, question_embed,\n",
    "\t\t\t\t\t best_val_acc):\n",
    "\tif val_accuracy > best_val_acc:\n",
    "\t\tbest_val_acc = val_accuracy\n",
    "\n",
    "\t\tfor file in os.listdir(experiment_path):\n",
    "\t\t\tif file.endswith('.pt'):\n",
    "\t\t\t\tos.remove(os.path.join(experiment_path, file))\n",
    "\n",
    "\t\tmodel_filename = f'epoch{epoch}_val_loss{avg_val_loss:.5f}_val_acc{val_accuracy:.5f}.pt'\n",
    "\t\tmodel_save_path = os.path.join(experiment_path, model_filename)\n",
    "\t\ttorch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "\t\ttorch.save(student_embed, os.path.join(experiment_path, 'student_embed.pt'))\n",
    "\t\ttorch.save(question_embed, os.path.join(experiment_path, 'question_embed.pt'))\n",
    "\n",
    "\treturn best_val_acc\n",
    "\n",
    "def train_model(\n",
    "\tn_students,\n",
    "\tn_questions,\n",
    "\ttrain_df,\n",
    "\tval_df,\n",
    "\tstudent_meta_tensor,\n",
    "\tstudent_embed_dim=8,\n",
    "\tquestion_embed_dim=16,\n",
    "\thidden_layers=[64, 16],\n",
    "\tdropout_p=0.3,\n",
    "\tbatch_size=32,\n",
    "\tlearning_rate=1e-3,\n",
    "\tdevice='cpu'\n",
    "):\n",
    "\tcheckpoint_dir = 'checkpoints'\n",
    "\tlog_dir = 'logs'\n",
    "\n",
    "\texperiment_num = 1\n",
    "\twhile os.path.exists(os.path.join(checkpoint_dir, f'experiment{experiment_num}')):\n",
    "\t\texperiment_num += 1\n",
    "\texperiment_path = os.path.join(checkpoint_dir, f'experiment{experiment_num}')\n",
    "\tos.makedirs(experiment_path, exist_ok=True)\n",
    "\n",
    "\thyperparameters = {\n",
    "\t\t'n_students': n_students,\n",
    "\t\t'n_questions': n_questions,\n",
    "\t\t'student_embed_dim': student_embed_dim,\n",
    "\t\t'question_embed_dim': question_embed_dim,\n",
    "\t\t'hidden_layers': hidden_layers,\n",
    "\t\t'dropout_p': dropout_p,\n",
    "\t\t'batch_size': batch_size,\n",
    "\t\t'learning_rate': learning_rate,\n",
    "\t\t'device': device\n",
    "\t}\n",
    "\twith open(os.path.join(experiment_path, 'hyperparameters.json'), 'w') as f:\n",
    "\t\tjson.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "\twriter = SummaryWriter(os.path.join(log_dir, f'experiment{experiment_num}'))\n",
    "\ttrain_dataloader, val_dataloader = initialize_dataloaders(train_df, val_df, batch_size)\n",
    "\n",
    "\tmodel, student_embed, question_embed, model_optimizer, embed_optimizer = initialize_model_and_optimizers(\n",
    "\t\tn_students, n_questions, student_embed_dim, question_embed_dim, student_meta_tensor.shape[1],\n",
    "\t\thidden_layers, dropout_p, learning_rate, device\n",
    "\t)\n",
    "\n",
    "\tdummy_student_embed = nn.Parameter(torch.randn(32, student_embed_dim)).to(device)\n",
    "\tdummy_question_embed = nn.Parameter(torch.randn(32, question_embed_dim)).to(device)\n",
    "\tdummy_student_meta = nn.Parameter(torch.randn(32, student_meta_tensor.shape[1])).to(device)\n",
    "\twriter.add_graph(model, (dummy_student_embed, dummy_question_embed, dummy_student_meta))\n",
    "\n",
    "\tcriterion = torch.nn.BCEWithLogitsLoss()\n",
    "\tbest_val_acc = -1\n",
    "\tepoch = 0\n",
    "\n",
    "\ttry:\n",
    "\t\twhile True:\n",
    "\t\t\tepoch += 1\n",
    "\n",
    "\t\t\tavg_train_loss, train_accuracy = train_step(\n",
    "\t\t\t\tmodel, train_dataloader, student_embed, question_embed, student_meta_tensor,\n",
    "\t\t\t\tcriterion, model_optimizer, embed_optimizer, device\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tavg_val_loss, val_accuracy = val_step(\n",
    "\t\t\t\tmodel, val_dataloader, student_embed, question_embed, student_meta_tensor,\n",
    "\t\t\t\tcriterion, device\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\twriter.add_scalar('Train/Loss', avg_train_loss, epoch)\n",
    "\t\t\twriter.add_scalar('Train/Accuracy', train_accuracy, epoch)\n",
    "\t\t\twriter.add_scalar('Validation/Loss', avg_val_loss, epoch)\n",
    "\t\t\twriter.add_scalar('Validation/Accuracy', val_accuracy, epoch)\n",
    "\n",
    "\t\t\tprint(f\"Epoch: {epoch} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Loss: {avg_val_loss:.5f} | Val Acc: {val_accuracy:.5f}\")\n",
    "\n",
    "\t\t\tbest_val_acc = checkpoint(\n",
    "\t\t\t\tepoch, experiment_path, avg_val_loss, val_accuracy, model, student_embed, question_embed, best_val_acc\n",
    "\t\t\t)\n",
    "\n",
    "\texcept KeyboardInterrupt:\n",
    "\t\tprint(\"early stopping\")\n",
    "\tfinally:\n",
    "\t\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 0.6869 | Train Acc: 0.5563 | Val Loss: 0.67433 | Val Acc: 0.60090\n",
      "Epoch: 2 | Train Loss: 0.6770 | Train Acc: 0.5865 | Val Loss: 0.67058 | Val Acc: 0.60048\n",
      "Epoch: 3 | Train Loss: 0.6731 | Train Acc: 0.5947 | Val Loss: 0.66808 | Val Acc: 0.60090\n",
      "Epoch: 4 | Train Loss: 0.6704 | Train Acc: 0.5964 | Val Loss: 0.66563 | Val Acc: 0.60189\n",
      "Epoch: 5 | Train Loss: 0.6670 | Train Acc: 0.6008 | Val Loss: 0.66304 | Val Acc: 0.60344\n",
      "Epoch: 6 | Train Loss: 0.6643 | Train Acc: 0.6014 | Val Loss: 0.66040 | Val Acc: 0.60739\n",
      "Epoch: 7 | Train Loss: 0.6613 | Train Acc: 0.6059 | Val Loss: 0.65658 | Val Acc: 0.60979\n",
      "Epoch: 8 | Train Loss: 0.6586 | Train Acc: 0.6087 | Val Loss: 0.65342 | Val Acc: 0.61304\n",
      "Epoch: 9 | Train Loss: 0.6561 | Train Acc: 0.6109 | Val Loss: 0.65109 | Val Acc: 0.62108\n",
      "Epoch: 10 | Train Loss: 0.6524 | Train Acc: 0.6170 | Val Loss: 0.64717 | Val Acc: 0.62038\n",
      "Epoch: 11 | Train Loss: 0.6511 | Train Acc: 0.6167 | Val Loss: 0.64412 | Val Acc: 0.62856\n",
      "Epoch: 12 | Train Loss: 0.6475 | Train Acc: 0.6218 | Val Loss: 0.64161 | Val Acc: 0.63364\n",
      "Epoch: 13 | Train Loss: 0.6443 | Train Acc: 0.6269 | Val Loss: 0.63870 | Val Acc: 0.63350\n",
      "Epoch: 14 | Train Loss: 0.6402 | Train Acc: 0.6314 | Val Loss: 0.63521 | Val Acc: 0.63576\n",
      "Epoch: 15 | Train Loss: 0.6392 | Train Acc: 0.6313 | Val Loss: 0.63279 | Val Acc: 0.63717\n",
      "Epoch: 16 | Train Loss: 0.6358 | Train Acc: 0.6392 | Val Loss: 0.62952 | Val Acc: 0.63858\n",
      "Epoch: 17 | Train Loss: 0.6336 | Train Acc: 0.6409 | Val Loss: 0.62722 | Val Acc: 0.64253\n",
      "Epoch: 18 | Train Loss: 0.6299 | Train Acc: 0.6444 | Val Loss: 0.62431 | Val Acc: 0.64409\n",
      "Epoch: 19 | Train Loss: 0.6287 | Train Acc: 0.6466 | Val Loss: 0.62291 | Val Acc: 0.64564\n",
      "Epoch: 20 | Train Loss: 0.6264 | Train Acc: 0.6487 | Val Loss: 0.61920 | Val Acc: 0.64804\n",
      "Epoch: 21 | Train Loss: 0.6244 | Train Acc: 0.6516 | Val Loss: 0.61909 | Val Acc: 0.64987\n",
      "Epoch: 22 | Train Loss: 0.6205 | Train Acc: 0.6532 | Val Loss: 0.61512 | Val Acc: 0.65284\n",
      "Epoch: 23 | Train Loss: 0.6198 | Train Acc: 0.6547 | Val Loss: 0.61384 | Val Acc: 0.65509\n",
      "Epoch: 24 | Train Loss: 0.6179 | Train Acc: 0.6584 | Val Loss: 0.61135 | Val Acc: 0.65721\n",
      "Epoch: 25 | Train Loss: 0.6148 | Train Acc: 0.6607 | Val Loss: 0.60943 | Val Acc: 0.65890\n",
      "Epoch: 26 | Train Loss: 0.6131 | Train Acc: 0.6621 | Val Loss: 0.60955 | Val Acc: 0.66116\n",
      "Epoch: 27 | Train Loss: 0.6109 | Train Acc: 0.6657 | Val Loss: 0.60573 | Val Acc: 0.66159\n",
      "Epoch: 28 | Train Loss: 0.6085 | Train Acc: 0.6677 | Val Loss: 0.60392 | Val Acc: 0.66399\n",
      "Epoch: 29 | Train Loss: 0.6060 | Train Acc: 0.6700 | Val Loss: 0.60162 | Val Acc: 0.66653\n",
      "Epoch: 30 | Train Loss: 0.6057 | Train Acc: 0.6707 | Val Loss: 0.60055 | Val Acc: 0.67062\n",
      "Epoch: 31 | Train Loss: 0.6036 | Train Acc: 0.6728 | Val Loss: 0.60038 | Val Acc: 0.67104\n",
      "Epoch: 32 | Train Loss: 0.6015 | Train Acc: 0.6741 | Val Loss: 0.59856 | Val Acc: 0.66977\n",
      "Epoch: 33 | Train Loss: 0.5998 | Train Acc: 0.6769 | Val Loss: 0.59644 | Val Acc: 0.67161\n",
      "Epoch: 34 | Train Loss: 0.5984 | Train Acc: 0.6802 | Val Loss: 0.59484 | Val Acc: 0.67499\n",
      "Epoch: 35 | Train Loss: 0.5975 | Train Acc: 0.6799 | Val Loss: 0.59352 | Val Acc: 0.67683\n",
      "Epoch: 36 | Train Loss: 0.5943 | Train Acc: 0.6820 | Val Loss: 0.59200 | Val Acc: 0.67880\n",
      "Epoch: 37 | Train Loss: 0.5943 | Train Acc: 0.6823 | Val Loss: 0.59186 | Val Acc: 0.68078\n",
      "Epoch: 38 | Train Loss: 0.5910 | Train Acc: 0.6861 | Val Loss: 0.58976 | Val Acc: 0.68078\n",
      "Epoch: 39 | Train Loss: 0.5892 | Train Acc: 0.6879 | Val Loss: 0.58915 | Val Acc: 0.68163\n",
      "Epoch: 40 | Train Loss: 0.5884 | Train Acc: 0.6883 | Val Loss: 0.58803 | Val Acc: 0.68459\n",
      "Epoch: 41 | Train Loss: 0.5870 | Train Acc: 0.6924 | Val Loss: 0.58865 | Val Acc: 0.68360\n",
      "Epoch: 42 | Train Loss: 0.5854 | Train Acc: 0.6911 | Val Loss: 0.58554 | Val Acc: 0.68572\n",
      "Epoch: 43 | Train Loss: 0.5836 | Train Acc: 0.6924 | Val Loss: 0.58486 | Val Acc: 0.68798\n",
      "Epoch: 44 | Train Loss: 0.5813 | Train Acc: 0.6958 | Val Loss: 0.58635 | Val Acc: 0.68374\n",
      "Epoch: 45 | Train Loss: 0.5807 | Train Acc: 0.6964 | Val Loss: 0.58385 | Val Acc: 0.68798\n",
      "Epoch: 46 | Train Loss: 0.5808 | Train Acc: 0.6952 | Val Loss: 0.58419 | Val Acc: 0.68529\n",
      "Epoch: 47 | Train Loss: 0.5784 | Train Acc: 0.6981 | Val Loss: 0.58136 | Val Acc: 0.68911\n",
      "Epoch: 48 | Train Loss: 0.5762 | Train Acc: 0.7007 | Val Loss: 0.58046 | Val Acc: 0.69023\n",
      "Epoch: 49 | Train Loss: 0.5755 | Train Acc: 0.7002 | Val Loss: 0.58031 | Val Acc: 0.68671\n",
      "Epoch: 50 | Train Loss: 0.5751 | Train Acc: 0.7006 | Val Loss: 0.57923 | Val Acc: 0.69038\n",
      "Epoch: 51 | Train Loss: 0.5748 | Train Acc: 0.7019 | Val Loss: 0.57821 | Val Acc: 0.69023\n",
      "Epoch: 52 | Train Loss: 0.5727 | Train Acc: 0.7052 | Val Loss: 0.57732 | Val Acc: 0.68925\n",
      "Epoch: 53 | Train Loss: 0.5707 | Train Acc: 0.7058 | Val Loss: 0.57671 | Val Acc: 0.69052\n",
      "Epoch: 54 | Train Loss: 0.5688 | Train Acc: 0.7073 | Val Loss: 0.57754 | Val Acc: 0.69038\n",
      "Epoch: 55 | Train Loss: 0.5690 | Train Acc: 0.7055 | Val Loss: 0.57521 | Val Acc: 0.69122\n",
      "Epoch: 56 | Train Loss: 0.5670 | Train Acc: 0.7087 | Val Loss: 0.57472 | Val Acc: 0.69235\n",
      "Epoch: 57 | Train Loss: 0.5660 | Train Acc: 0.7094 | Val Loss: 0.57555 | Val Acc: 0.69306\n",
      "Epoch: 58 | Train Loss: 0.5657 | Train Acc: 0.7083 | Val Loss: 0.57300 | Val Acc: 0.69503\n",
      "Epoch: 59 | Train Loss: 0.5633 | Train Acc: 0.7098 | Val Loss: 0.57355 | Val Acc: 0.69447\n",
      "Epoch: 60 | Train Loss: 0.5634 | Train Acc: 0.7107 | Val Loss: 0.57239 | Val Acc: 0.69602\n",
      "Epoch: 61 | Train Loss: 0.5630 | Train Acc: 0.7101 | Val Loss: 0.57187 | Val Acc: 0.69560\n",
      "Epoch: 62 | Train Loss: 0.5620 | Train Acc: 0.7122 | Val Loss: 0.57048 | Val Acc: 0.69292\n",
      "Epoch: 63 | Train Loss: 0.5596 | Train Acc: 0.7143 | Val Loss: 0.57072 | Val Acc: 0.69574\n",
      "Epoch: 64 | Train Loss: 0.5605 | Train Acc: 0.7139 | Val Loss: 0.57104 | Val Acc: 0.69673\n",
      "Epoch: 65 | Train Loss: 0.5594 | Train Acc: 0.7152 | Val Loss: 0.57031 | Val Acc: 0.69489\n",
      "Epoch: 66 | Train Loss: 0.5588 | Train Acc: 0.7157 | Val Loss: 0.57149 | Val Acc: 0.69814\n",
      "Epoch: 67 | Train Loss: 0.5578 | Train Acc: 0.7171 | Val Loss: 0.56995 | Val Acc: 0.69842\n",
      "Epoch: 68 | Train Loss: 0.5569 | Train Acc: 0.7190 | Val Loss: 0.56893 | Val Acc: 0.69814\n",
      "Epoch: 69 | Train Loss: 0.5560 | Train Acc: 0.7181 | Val Loss: 0.56858 | Val Acc: 0.69941\n",
      "Epoch: 70 | Train Loss: 0.5569 | Train Acc: 0.7172 | Val Loss: 0.56837 | Val Acc: 0.69687\n",
      "Epoch: 71 | Train Loss: 0.5538 | Train Acc: 0.7166 | Val Loss: 0.56861 | Val Acc: 0.69715\n",
      "Epoch: 72 | Train Loss: 0.5536 | Train Acc: 0.7190 | Val Loss: 0.56713 | Val Acc: 0.69955\n",
      "Epoch: 73 | Train Loss: 0.5535 | Train Acc: 0.7185 | Val Loss: 0.56686 | Val Acc: 0.69856\n",
      "Epoch: 74 | Train Loss: 0.5525 | Train Acc: 0.7196 | Val Loss: 0.56653 | Val Acc: 0.69955\n",
      "Epoch: 75 | Train Loss: 0.5523 | Train Acc: 0.7194 | Val Loss: 0.56662 | Val Acc: 0.70152\n",
      "Epoch: 76 | Train Loss: 0.5519 | Train Acc: 0.7231 | Val Loss: 0.56566 | Val Acc: 0.69941\n",
      "Epoch: 77 | Train Loss: 0.5483 | Train Acc: 0.7229 | Val Loss: 0.56868 | Val Acc: 0.70054\n",
      "Epoch: 78 | Train Loss: 0.5496 | Train Acc: 0.7221 | Val Loss: 0.56560 | Val Acc: 0.70167\n",
      "Epoch: 79 | Train Loss: 0.5484 | Train Acc: 0.7235 | Val Loss: 0.56599 | Val Acc: 0.70167\n",
      "Epoch: 80 | Train Loss: 0.5487 | Train Acc: 0.7236 | Val Loss: 0.56573 | Val Acc: 0.70054\n",
      "Epoch: 81 | Train Loss: 0.5485 | Train Acc: 0.7222 | Val Loss: 0.56647 | Val Acc: 0.70209\n",
      "Epoch: 82 | Train Loss: 0.5488 | Train Acc: 0.7222 | Val Loss: 0.56450 | Val Acc: 0.70167\n",
      "Epoch: 83 | Train Loss: 0.5481 | Train Acc: 0.7247 | Val Loss: 0.56418 | Val Acc: 0.70336\n",
      "Epoch: 84 | Train Loss: 0.5460 | Train Acc: 0.7245 | Val Loss: 0.56487 | Val Acc: 0.70294\n",
      "Epoch: 85 | Train Loss: 0.5458 | Train Acc: 0.7252 | Val Loss: 0.56400 | Val Acc: 0.70237\n",
      "Epoch: 86 | Train Loss: 0.5449 | Train Acc: 0.7262 | Val Loss: 0.56366 | Val Acc: 0.70294\n",
      "Epoch: 87 | Train Loss: 0.5463 | Train Acc: 0.7249 | Val Loss: 0.56444 | Val Acc: 0.70138\n",
      "Epoch: 88 | Train Loss: 0.5436 | Train Acc: 0.7274 | Val Loss: 0.56299 | Val Acc: 0.70054\n",
      "Epoch: 89 | Train Loss: 0.5462 | Train Acc: 0.7270 | Val Loss: 0.56410 | Val Acc: 0.70110\n",
      "Epoch: 90 | Train Loss: 0.5429 | Train Acc: 0.7293 | Val Loss: 0.56281 | Val Acc: 0.70025\n",
      "Epoch: 91 | Train Loss: 0.5440 | Train Acc: 0.7260 | Val Loss: 0.56404 | Val Acc: 0.70195\n",
      "Epoch: 92 | Train Loss: 0.5437 | Train Acc: 0.7267 | Val Loss: 0.56279 | Val Acc: 0.70096\n",
      "Epoch: 93 | Train Loss: 0.5421 | Train Acc: 0.7301 | Val Loss: 0.56239 | Val Acc: 0.70223\n",
      "Epoch: 94 | Train Loss: 0.5411 | Train Acc: 0.7290 | Val Loss: 0.56234 | Val Acc: 0.70096\n",
      "Epoch: 95 | Train Loss: 0.5406 | Train Acc: 0.7294 | Val Loss: 0.56172 | Val Acc: 0.70251\n",
      "Epoch: 96 | Train Loss: 0.5408 | Train Acc: 0.7312 | Val Loss: 0.56192 | Val Acc: 0.70378\n",
      "Epoch: 97 | Train Loss: 0.5394 | Train Acc: 0.7294 | Val Loss: 0.56227 | Val Acc: 0.70519\n",
      "Epoch: 98 | Train Loss: 0.5396 | Train Acc: 0.7299 | Val Loss: 0.56211 | Val Acc: 0.70209\n",
      "Epoch: 99 | Train Loss: 0.5398 | Train Acc: 0.7310 | Val Loss: 0.56138 | Val Acc: 0.70195\n",
      "Epoch: 100 | Train Loss: 0.5386 | Train Acc: 0.7309 | Val Loss: 0.56132 | Val Acc: 0.70491\n",
      "Epoch: 101 | Train Loss: 0.5381 | Train Acc: 0.7315 | Val Loss: 0.56088 | Val Acc: 0.70195\n",
      "Epoch: 102 | Train Loss: 0.5385 | Train Acc: 0.7316 | Val Loss: 0.56171 | Val Acc: 0.70279\n",
      "Epoch: 103 | Train Loss: 0.5372 | Train Acc: 0.7326 | Val Loss: 0.55999 | Val Acc: 0.70364\n",
      "Epoch: 104 | Train Loss: 0.5374 | Train Acc: 0.7316 | Val Loss: 0.56143 | Val Acc: 0.70364\n",
      "Epoch: 105 | Train Loss: 0.5373 | Train Acc: 0.7339 | Val Loss: 0.56249 | Val Acc: 0.70491\n",
      "Epoch: 106 | Train Loss: 0.5371 | Train Acc: 0.7347 | Val Loss: 0.56064 | Val Acc: 0.70392\n",
      "Epoch: 107 | Train Loss: 0.5352 | Train Acc: 0.7350 | Val Loss: 0.56300 | Val Acc: 0.70336\n",
      "Epoch: 108 | Train Loss: 0.5370 | Train Acc: 0.7332 | Val Loss: 0.56170 | Val Acc: 0.70068\n",
      "Epoch: 109 | Train Loss: 0.5353 | Train Acc: 0.7339 | Val Loss: 0.56201 | Val Acc: 0.70491\n",
      "Epoch: 110 | Train Loss: 0.5360 | Train Acc: 0.7334 | Val Loss: 0.56078 | Val Acc: 0.70406\n",
      "Epoch: 111 | Train Loss: 0.5336 | Train Acc: 0.7351 | Val Loss: 0.56150 | Val Acc: 0.70477\n",
      "Epoch: 112 | Train Loss: 0.5346 | Train Acc: 0.7344 | Val Loss: 0.56023 | Val Acc: 0.70562\n",
      "Epoch: 113 | Train Loss: 0.5349 | Train Acc: 0.7363 | Val Loss: 0.56125 | Val Acc: 0.70816\n",
      "Epoch: 114 | Train Loss: 0.5342 | Train Acc: 0.7363 | Val Loss: 0.56072 | Val Acc: 0.70463\n",
      "Epoch: 115 | Train Loss: 0.5341 | Train Acc: 0.7358 | Val Loss: 0.56001 | Val Acc: 0.70322\n",
      "Epoch: 116 | Train Loss: 0.5332 | Train Acc: 0.7375 | Val Loss: 0.56059 | Val Acc: 0.70689\n",
      "Epoch: 117 | Train Loss: 0.5336 | Train Acc: 0.7373 | Val Loss: 0.55934 | Val Acc: 0.70590\n",
      "Epoch: 118 | Train Loss: 0.5323 | Train Acc: 0.7359 | Val Loss: 0.55933 | Val Acc: 0.70477\n",
      "Epoch: 119 | Train Loss: 0.5324 | Train Acc: 0.7381 | Val Loss: 0.56016 | Val Acc: 0.70505\n",
      "Epoch: 120 | Train Loss: 0.5321 | Train Acc: 0.7364 | Val Loss: 0.56084 | Val Acc: 0.70759\n",
      "Epoch: 121 | Train Loss: 0.5320 | Train Acc: 0.7373 | Val Loss: 0.56010 | Val Acc: 0.70435\n",
      "Epoch: 122 | Train Loss: 0.5325 | Train Acc: 0.7373 | Val Loss: 0.56016 | Val Acc: 0.70491\n",
      "Epoch: 123 | Train Loss: 0.5317 | Train Acc: 0.7376 | Val Loss: 0.56031 | Val Acc: 0.70449\n",
      "Epoch: 124 | Train Loss: 0.5308 | Train Acc: 0.7370 | Val Loss: 0.55927 | Val Acc: 0.70435\n",
      "Epoch: 125 | Train Loss: 0.5303 | Train Acc: 0.7388 | Val Loss: 0.56061 | Val Acc: 0.70138\n",
      "Epoch: 126 | Train Loss: 0.5329 | Train Acc: 0.7369 | Val Loss: 0.55902 | Val Acc: 0.70505\n",
      "Epoch: 127 | Train Loss: 0.5310 | Train Acc: 0.7382 | Val Loss: 0.55916 | Val Acc: 0.70378\n",
      "Epoch: 128 | Train Loss: 0.5318 | Train Acc: 0.7365 | Val Loss: 0.56017 | Val Acc: 0.70604\n",
      "Epoch: 129 | Train Loss: 0.5287 | Train Acc: 0.7385 | Val Loss: 0.55922 | Val Acc: 0.70548\n",
      "Epoch: 130 | Train Loss: 0.5299 | Train Acc: 0.7386 | Val Loss: 0.55955 | Val Acc: 0.70421\n",
      "Epoch: 131 | Train Loss: 0.5297 | Train Acc: 0.7399 | Val Loss: 0.56025 | Val Acc: 0.70703\n",
      "Epoch: 132 | Train Loss: 0.5301 | Train Acc: 0.7389 | Val Loss: 0.56067 | Val Acc: 0.70576\n",
      "Epoch: 133 | Train Loss: 0.5292 | Train Acc: 0.7388 | Val Loss: 0.56035 | Val Acc: 0.70505\n",
      "Epoch: 134 | Train Loss: 0.5295 | Train Acc: 0.7390 | Val Loss: 0.55902 | Val Acc: 0.70519\n",
      "Epoch: 135 | Train Loss: 0.5299 | Train Acc: 0.7385 | Val Loss: 0.55972 | Val Acc: 0.70604\n",
      "Epoch: 136 | Train Loss: 0.5287 | Train Acc: 0.7386 | Val Loss: 0.55946 | Val Acc: 0.70576\n",
      "Epoch: 137 | Train Loss: 0.5280 | Train Acc: 0.7402 | Val Loss: 0.55942 | Val Acc: 0.70421\n",
      "Epoch: 138 | Train Loss: 0.5288 | Train Acc: 0.7384 | Val Loss: 0.55957 | Val Acc: 0.70675\n",
      "Epoch: 139 | Train Loss: 0.5270 | Train Acc: 0.7407 | Val Loss: 0.55871 | Val Acc: 0.70548\n",
      "Epoch: 140 | Train Loss: 0.5288 | Train Acc: 0.7387 | Val Loss: 0.55967 | Val Acc: 0.70604\n",
      "Epoch: 141 | Train Loss: 0.5256 | Train Acc: 0.7410 | Val Loss: 0.55889 | Val Acc: 0.70590\n",
      "Epoch: 142 | Train Loss: 0.5275 | Train Acc: 0.7414 | Val Loss: 0.55959 | Val Acc: 0.70844\n",
      "Epoch: 143 | Train Loss: 0.5265 | Train Acc: 0.7400 | Val Loss: 0.56006 | Val Acc: 0.70759\n",
      "Epoch: 144 | Train Loss: 0.5276 | Train Acc: 0.7427 | Val Loss: 0.55894 | Val Acc: 0.70689\n",
      "Epoch: 145 | Train Loss: 0.5259 | Train Acc: 0.7412 | Val Loss: 0.56002 | Val Acc: 0.70703\n",
      "Epoch: 146 | Train Loss: 0.5273 | Train Acc: 0.7409 | Val Loss: 0.56087 | Val Acc: 0.70533\n",
      "Epoch: 147 | Train Loss: 0.5258 | Train Acc: 0.7411 | Val Loss: 0.55863 | Val Acc: 0.70618\n",
      "Epoch: 148 | Train Loss: 0.5260 | Train Acc: 0.7397 | Val Loss: 0.55928 | Val Acc: 0.70463\n",
      "Epoch: 149 | Train Loss: 0.5263 | Train Acc: 0.7407 | Val Loss: 0.55915 | Val Acc: 0.70689\n",
      "Epoch: 150 | Train Loss: 0.5257 | Train Acc: 0.7392 | Val Loss: 0.55921 | Val Acc: 0.70787\n",
      "Epoch: 151 | Train Loss: 0.5265 | Train Acc: 0.7404 | Val Loss: 0.56139 | Val Acc: 0.70943\n",
      "Epoch: 152 | Train Loss: 0.5229 | Train Acc: 0.7433 | Val Loss: 0.55928 | Val Acc: 0.70703\n",
      "Epoch: 153 | Train Loss: 0.5250 | Train Acc: 0.7434 | Val Loss: 0.55904 | Val Acc: 0.70519\n",
      "Epoch: 154 | Train Loss: 0.5260 | Train Acc: 0.7423 | Val Loss: 0.55884 | Val Acc: 0.70632\n",
      "Epoch: 155 | Train Loss: 0.5253 | Train Acc: 0.7417 | Val Loss: 0.56013 | Val Acc: 0.70491\n",
      "Epoch: 156 | Train Loss: 0.5246 | Train Acc: 0.7427 | Val Loss: 0.56069 | Val Acc: 0.70872\n",
      "Epoch: 157 | Train Loss: 0.5239 | Train Acc: 0.7427 | Val Loss: 0.55926 | Val Acc: 0.70209\n",
      "Epoch: 158 | Train Loss: 0.5220 | Train Acc: 0.7440 | Val Loss: 0.55983 | Val Acc: 0.70562\n",
      "Epoch: 159 | Train Loss: 0.5247 | Train Acc: 0.7435 | Val Loss: 0.55932 | Val Acc: 0.70632\n",
      "Epoch: 160 | Train Loss: 0.5220 | Train Acc: 0.7435 | Val Loss: 0.55964 | Val Acc: 0.70689\n",
      "Epoch: 161 | Train Loss: 0.5245 | Train Acc: 0.7415 | Val Loss: 0.56083 | Val Acc: 0.70477\n",
      "Epoch: 162 | Train Loss: 0.5219 | Train Acc: 0.7442 | Val Loss: 0.56013 | Val Acc: 0.70590\n",
      "Epoch: 163 | Train Loss: 0.5222 | Train Acc: 0.7436 | Val Loss: 0.56001 | Val Acc: 0.70477\n",
      "Epoch: 164 | Train Loss: 0.5216 | Train Acc: 0.7437 | Val Loss: 0.56041 | Val Acc: 0.70717\n",
      "Epoch: 165 | Train Loss: 0.5226 | Train Acc: 0.7453 | Val Loss: 0.56049 | Val Acc: 0.70773\n",
      "Epoch: 166 | Train Loss: 0.5235 | Train Acc: 0.7435 | Val Loss: 0.56094 | Val Acc: 0.70660\n",
      "Epoch: 167 | Train Loss: 0.5230 | Train Acc: 0.7445 | Val Loss: 0.56100 | Val Acc: 0.70294\n",
      "Epoch: 168 | Train Loss: 0.5217 | Train Acc: 0.7450 | Val Loss: 0.56074 | Val Acc: 0.70590\n",
      "Epoch: 169 | Train Loss: 0.5236 | Train Acc: 0.7431 | Val Loss: 0.56281 | Val Acc: 0.70138\n",
      "Epoch: 170 | Train Loss: 0.5223 | Train Acc: 0.7426 | Val Loss: 0.56077 | Val Acc: 0.70618\n",
      "Epoch: 171 | Train Loss: 0.5209 | Train Acc: 0.7452 | Val Loss: 0.56142 | Val Acc: 0.70505\n",
      "Epoch: 172 | Train Loss: 0.5207 | Train Acc: 0.7432 | Val Loss: 0.56104 | Val Acc: 0.70336\n",
      "Epoch: 173 | Train Loss: 0.5204 | Train Acc: 0.7452 | Val Loss: 0.56385 | Val Acc: 0.70717\n",
      "Epoch: 174 | Train Loss: 0.5199 | Train Acc: 0.7450 | Val Loss: 0.56079 | Val Acc: 0.70703\n",
      "Epoch: 175 | Train Loss: 0.5200 | Train Acc: 0.7450 | Val Loss: 0.56167 | Val Acc: 0.70576\n",
      "Epoch: 176 | Train Loss: 0.5187 | Train Acc: 0.7468 | Val Loss: 0.55984 | Val Acc: 0.70632\n",
      "Epoch: 177 | Train Loss: 0.5206 | Train Acc: 0.7456 | Val Loss: 0.56051 | Val Acc: 0.70477\n",
      "Epoch: 178 | Train Loss: 0.5187 | Train Acc: 0.7452 | Val Loss: 0.56133 | Val Acc: 0.70477\n",
      "Epoch: 179 | Train Loss: 0.5194 | Train Acc: 0.7459 | Val Loss: 0.56148 | Val Acc: 0.70929\n",
      "Epoch: 180 | Train Loss: 0.5204 | Train Acc: 0.7447 | Val Loss: 0.55980 | Val Acc: 0.70505\n",
      "Epoch: 181 | Train Loss: 0.5201 | Train Acc: 0.7453 | Val Loss: 0.56469 | Val Acc: 0.70590\n",
      "Epoch: 182 | Train Loss: 0.5188 | Train Acc: 0.7457 | Val Loss: 0.55946 | Val Acc: 0.70830\n",
      "Epoch: 183 | Train Loss: 0.5192 | Train Acc: 0.7458 | Val Loss: 0.56048 | Val Acc: 0.70590\n",
      "Epoch: 184 | Train Loss: 0.5200 | Train Acc: 0.7459 | Val Loss: 0.55998 | Val Acc: 0.70548\n",
      "Epoch: 185 | Train Loss: 0.5174 | Train Acc: 0.7465 | Val Loss: 0.56232 | Val Acc: 0.70533\n",
      "Epoch: 186 | Train Loss: 0.5191 | Train Acc: 0.7454 | Val Loss: 0.56196 | Val Acc: 0.70435\n",
      "Epoch: 187 | Train Loss: 0.5191 | Train Acc: 0.7457 | Val Loss: 0.56197 | Val Acc: 0.70787\n",
      "Epoch: 188 | Train Loss: 0.5212 | Train Acc: 0.7429 | Val Loss: 0.56198 | Val Acc: 0.70787\n",
      "Epoch: 189 | Train Loss: 0.5182 | Train Acc: 0.7448 | Val Loss: 0.56345 | Val Acc: 0.70477\n",
      "Epoch: 190 | Train Loss: 0.5185 | Train Acc: 0.7438 | Val Loss: 0.56062 | Val Acc: 0.70703\n",
      "Epoch: 191 | Train Loss: 0.5178 | Train Acc: 0.7475 | Val Loss: 0.56158 | Val Acc: 0.70519\n",
      "Epoch: 192 | Train Loss: 0.5173 | Train Acc: 0.7468 | Val Loss: 0.56288 | Val Acc: 0.70350\n",
      "Epoch: 193 | Train Loss: 0.5188 | Train Acc: 0.7453 | Val Loss: 0.56281 | Val Acc: 0.70802\n",
      "Epoch: 194 | Train Loss: 0.5173 | Train Acc: 0.7473 | Val Loss: 0.56222 | Val Acc: 0.70773\n",
      "Epoch: 195 | Train Loss: 0.5168 | Train Acc: 0.7479 | Val Loss: 0.56303 | Val Acc: 0.70703\n",
      "Epoch: 196 | Train Loss: 0.5175 | Train Acc: 0.7465 | Val Loss: 0.56080 | Val Acc: 0.70731\n",
      "Epoch: 197 | Train Loss: 0.5162 | Train Acc: 0.7485 | Val Loss: 0.56232 | Val Acc: 0.70463\n",
      "Epoch: 198 | Train Loss: 0.5168 | Train Acc: 0.7472 | Val Loss: 0.56228 | Val Acc: 0.70562\n",
      "Epoch: 199 | Train Loss: 0.5154 | Train Acc: 0.7489 | Val Loss: 0.56551 | Val Acc: 0.70576\n",
      "Epoch: 200 | Train Loss: 0.5160 | Train Acc: 0.7478 | Val Loss: 0.56210 | Val Acc: 0.70505\n",
      "Epoch: 201 | Train Loss: 0.5165 | Train Acc: 0.7459 | Val Loss: 0.56424 | Val Acc: 0.70533\n",
      "Epoch: 202 | Train Loss: 0.5158 | Train Acc: 0.7474 | Val Loss: 0.56313 | Val Acc: 0.70378\n",
      "Epoch: 203 | Train Loss: 0.5163 | Train Acc: 0.7480 | Val Loss: 0.56435 | Val Acc: 0.70646\n",
      "Epoch: 204 | Train Loss: 0.5157 | Train Acc: 0.7486 | Val Loss: 0.56284 | Val Acc: 0.70251\n",
      "Epoch: 205 | Train Loss: 0.5145 | Train Acc: 0.7476 | Val Loss: 0.56393 | Val Acc: 0.70519\n",
      "Epoch: 206 | Train Loss: 0.5151 | Train Acc: 0.7472 | Val Loss: 0.56459 | Val Acc: 0.70449\n",
      "Epoch: 207 | Train Loss: 0.5148 | Train Acc: 0.7482 | Val Loss: 0.56546 | Val Acc: 0.70392\n",
      "Epoch: 208 | Train Loss: 0.5137 | Train Acc: 0.7496 | Val Loss: 0.56508 | Val Acc: 0.70279\n",
      "Epoch: 209 | Train Loss: 0.5144 | Train Acc: 0.7469 | Val Loss: 0.56492 | Val Acc: 0.70237\n",
      "Epoch: 210 | Train Loss: 0.5137 | Train Acc: 0.7476 | Val Loss: 0.56148 | Val Acc: 0.70435\n",
      "Epoch: 211 | Train Loss: 0.5148 | Train Acc: 0.7489 | Val Loss: 0.56593 | Val Acc: 0.70308\n",
      "Epoch: 212 | Train Loss: 0.5156 | Train Acc: 0.7461 | Val Loss: 0.56326 | Val Acc: 0.70406\n",
      "Epoch: 213 | Train Loss: 0.5138 | Train Acc: 0.7476 | Val Loss: 0.56150 | Val Acc: 0.70308\n",
      "Epoch: 214 | Train Loss: 0.5141 | Train Acc: 0.7508 | Val Loss: 0.56427 | Val Acc: 0.70491\n",
      "Epoch: 215 | Train Loss: 0.5123 | Train Acc: 0.7497 | Val Loss: 0.56402 | Val Acc: 0.70279\n",
      "Epoch: 216 | Train Loss: 0.5126 | Train Acc: 0.7486 | Val Loss: 0.56322 | Val Acc: 0.70421\n",
      "Epoch: 217 | Train Loss: 0.5118 | Train Acc: 0.7492 | Val Loss: 0.56315 | Val Acc: 0.70533\n",
      "Epoch: 218 | Train Loss: 0.5132 | Train Acc: 0.7482 | Val Loss: 0.56315 | Val Acc: 0.70491\n",
      "Epoch: 219 | Train Loss: 0.5108 | Train Acc: 0.7511 | Val Loss: 0.56360 | Val Acc: 0.70364\n",
      "Epoch: 220 | Train Loss: 0.5112 | Train Acc: 0.7502 | Val Loss: 0.56639 | Val Acc: 0.70406\n",
      "Epoch: 221 | Train Loss: 0.5125 | Train Acc: 0.7504 | Val Loss: 0.56298 | Val Acc: 0.70406\n",
      "Epoch: 222 | Train Loss: 0.5117 | Train Acc: 0.7506 | Val Loss: 0.56298 | Val Acc: 0.70308\n",
      "Epoch: 223 | Train Loss: 0.5106 | Train Acc: 0.7510 | Val Loss: 0.56447 | Val Acc: 0.70378\n",
      "Epoch: 224 | Train Loss: 0.5109 | Train Acc: 0.7499 | Val Loss: 0.56516 | Val Acc: 0.70491\n",
      "Epoch: 225 | Train Loss: 0.5118 | Train Acc: 0.7505 | Val Loss: 0.56402 | Val Acc: 0.70421\n",
      "Epoch: 226 | Train Loss: 0.5122 | Train Acc: 0.7501 | Val Loss: 0.56586 | Val Acc: 0.70364\n",
      "Epoch: 227 | Train Loss: 0.5107 | Train Acc: 0.7505 | Val Loss: 0.56376 | Val Acc: 0.70519\n",
      "Epoch: 228 | Train Loss: 0.5097 | Train Acc: 0.7502 | Val Loss: 0.56609 | Val Acc: 0.70336\n",
      "Epoch: 229 | Train Loss: 0.5095 | Train Acc: 0.7522 | Val Loss: 0.56536 | Val Acc: 0.70237\n",
      "Epoch: 230 | Train Loss: 0.5115 | Train Acc: 0.7493 | Val Loss: 0.56403 | Val Acc: 0.70364\n",
      "Epoch: 231 | Train Loss: 0.5093 | Train Acc: 0.7515 | Val Loss: 0.56394 | Val Acc: 0.70421\n",
      "Epoch: 232 | Train Loss: 0.5100 | Train Acc: 0.7502 | Val Loss: 0.56612 | Val Acc: 0.70505\n",
      "Epoch: 233 | Train Loss: 0.5105 | Train Acc: 0.7508 | Val Loss: 0.56877 | Val Acc: 0.70435\n",
      "Epoch: 234 | Train Loss: 0.5109 | Train Acc: 0.7502 | Val Loss: 0.56589 | Val Acc: 0.70265\n",
      "Epoch: 235 | Train Loss: 0.5092 | Train Acc: 0.7535 | Val Loss: 0.56617 | Val Acc: 0.70308\n",
      "Epoch: 236 | Train Loss: 0.5107 | Train Acc: 0.7511 | Val Loss: 0.56593 | Val Acc: 0.70519\n",
      "Epoch: 237 | Train Loss: 0.5108 | Train Acc: 0.7501 | Val Loss: 0.56509 | Val Acc: 0.70576\n",
      "Epoch: 238 | Train Loss: 0.5099 | Train Acc: 0.7520 | Val Loss: 0.56497 | Val Acc: 0.70378\n",
      "Epoch: 239 | Train Loss: 0.5099 | Train Acc: 0.7498 | Val Loss: 0.56661 | Val Acc: 0.70533\n",
      "Epoch: 240 | Train Loss: 0.5093 | Train Acc: 0.7521 | Val Loss: 0.56531 | Val Acc: 0.70336\n",
      "Epoch: 241 | Train Loss: 0.5100 | Train Acc: 0.7497 | Val Loss: 0.56487 | Val Acc: 0.70477\n",
      "Epoch: 242 | Train Loss: 0.5093 | Train Acc: 0.7511 | Val Loss: 0.56655 | Val Acc: 0.70350\n",
      "Epoch: 243 | Train Loss: 0.5088 | Train Acc: 0.7501 | Val Loss: 0.56878 | Val Acc: 0.70322\n",
      "Epoch: 244 | Train Loss: 0.5092 | Train Acc: 0.7500 | Val Loss: 0.56669 | Val Acc: 0.70533\n",
      "Epoch: 245 | Train Loss: 0.5076 | Train Acc: 0.7515 | Val Loss: 0.56587 | Val Acc: 0.70463\n",
      "Epoch: 246 | Train Loss: 0.5089 | Train Acc: 0.7505 | Val Loss: 0.56634 | Val Acc: 0.70364\n",
      "Epoch: 247 | Train Loss: 0.5089 | Train Acc: 0.7526 | Val Loss: 0.56463 | Val Acc: 0.70237\n",
      "Epoch: 248 | Train Loss: 0.5080 | Train Acc: 0.7518 | Val Loss: 0.56707 | Val Acc: 0.70251\n",
      "Epoch: 249 | Train Loss: 0.5098 | Train Acc: 0.7509 | Val Loss: 0.56629 | Val Acc: 0.70336\n",
      "Epoch: 250 | Train Loss: 0.5051 | Train Acc: 0.7539 | Val Loss: 0.56793 | Val Acc: 0.70378\n",
      "Epoch: 251 | Train Loss: 0.5065 | Train Acc: 0.7531 | Val Loss: 0.56725 | Val Acc: 0.70392\n",
      "Epoch: 252 | Train Loss: 0.5069 | Train Acc: 0.7535 | Val Loss: 0.56539 | Val Acc: 0.70336\n",
      "Epoch: 253 | Train Loss: 0.5063 | Train Acc: 0.7519 | Val Loss: 0.56655 | Val Acc: 0.70364\n",
      "Epoch: 254 | Train Loss: 0.5068 | Train Acc: 0.7526 | Val Loss: 0.56775 | Val Acc: 0.70308\n",
      "Epoch: 255 | Train Loss: 0.5073 | Train Acc: 0.7523 | Val Loss: 0.56839 | Val Acc: 0.70350\n",
      "Epoch: 256 | Train Loss: 0.5057 | Train Acc: 0.7527 | Val Loss: 0.56612 | Val Acc: 0.70279\n",
      "Epoch: 257 | Train Loss: 0.5052 | Train Acc: 0.7529 | Val Loss: 0.56746 | Val Acc: 0.70040\n",
      "Epoch: 258 | Train Loss: 0.5075 | Train Acc: 0.7521 | Val Loss: 0.56868 | Val Acc: 0.70251\n",
      "Epoch: 259 | Train Loss: 0.5064 | Train Acc: 0.7525 | Val Loss: 0.56552 | Val Acc: 0.70237\n",
      "Epoch: 260 | Train Loss: 0.5054 | Train Acc: 0.7532 | Val Loss: 0.56740 | Val Acc: 0.70378\n",
      "Epoch: 261 | Train Loss: 0.5024 | Train Acc: 0.7534 | Val Loss: 0.56803 | Val Acc: 0.70040\n",
      "Epoch: 262 | Train Loss: 0.5062 | Train Acc: 0.7539 | Val Loss: 0.56853 | Val Acc: 0.70251\n",
      "Epoch: 263 | Train Loss: 0.5042 | Train Acc: 0.7532 | Val Loss: 0.56877 | Val Acc: 0.70322\n",
      "Epoch: 264 | Train Loss: 0.5069 | Train Acc: 0.7523 | Val Loss: 0.56788 | Val Acc: 0.70562\n",
      "Epoch: 265 | Train Loss: 0.5046 | Train Acc: 0.7547 | Val Loss: 0.56991 | Val Acc: 0.70364\n",
      "Epoch: 266 | Train Loss: 0.5054 | Train Acc: 0.7556 | Val Loss: 0.56864 | Val Acc: 0.70477\n",
      "Epoch: 267 | Train Loss: 0.5064 | Train Acc: 0.7544 | Val Loss: 0.57105 | Val Acc: 0.70336\n",
      "Epoch: 268 | Train Loss: 0.5035 | Train Acc: 0.7553 | Val Loss: 0.56614 | Val Acc: 0.70533\n",
      "Epoch: 269 | Train Loss: 0.5022 | Train Acc: 0.7553 | Val Loss: 0.57003 | Val Acc: 0.70082\n",
      "Epoch: 270 | Train Loss: 0.5042 | Train Acc: 0.7534 | Val Loss: 0.56951 | Val Acc: 0.70138\n",
      "Epoch: 271 | Train Loss: 0.5039 | Train Acc: 0.7558 | Val Loss: 0.56738 | Val Acc: 0.69969\n",
      "Epoch: 272 | Train Loss: 0.5046 | Train Acc: 0.7532 | Val Loss: 0.56973 | Val Acc: 0.70322\n",
      "Epoch: 273 | Train Loss: 0.5026 | Train Acc: 0.7546 | Val Loss: 0.56890 | Val Acc: 0.70138\n",
      "Epoch: 274 | Train Loss: 0.5039 | Train Acc: 0.7546 | Val Loss: 0.56894 | Val Acc: 0.70138\n",
      "Epoch: 275 | Train Loss: 0.5024 | Train Acc: 0.7556 | Val Loss: 0.56777 | Val Acc: 0.70491\n",
      "Epoch: 276 | Train Loss: 0.5021 | Train Acc: 0.7538 | Val Loss: 0.57166 | Val Acc: 0.69785\n",
      "Epoch: 277 | Train Loss: 0.5024 | Train Acc: 0.7561 | Val Loss: 0.57270 | Val Acc: 0.69842\n",
      "Epoch: 278 | Train Loss: 0.5020 | Train Acc: 0.7551 | Val Loss: 0.57093 | Val Acc: 0.70223\n",
      "Epoch: 279 | Train Loss: 0.5022 | Train Acc: 0.7554 | Val Loss: 0.56797 | Val Acc: 0.70082\n",
      "Epoch: 280 | Train Loss: 0.5000 | Train Acc: 0.7571 | Val Loss: 0.56790 | Val Acc: 0.70308\n",
      "Epoch: 281 | Train Loss: 0.5005 | Train Acc: 0.7550 | Val Loss: 0.57146 | Val Acc: 0.70237\n",
      "Epoch: 282 | Train Loss: 0.5010 | Train Acc: 0.7561 | Val Loss: 0.57129 | Val Acc: 0.70068\n",
      "Epoch: 283 | Train Loss: 0.5039 | Train Acc: 0.7543 | Val Loss: 0.57011 | Val Acc: 0.70152\n",
      "Epoch: 284 | Train Loss: 0.5018 | Train Acc: 0.7560 | Val Loss: 0.57151 | Val Acc: 0.70096\n",
      "Epoch: 285 | Train Loss: 0.5025 | Train Acc: 0.7545 | Val Loss: 0.57242 | Val Acc: 0.70209\n",
      "Epoch: 286 | Train Loss: 0.5031 | Train Acc: 0.7530 | Val Loss: 0.57095 | Val Acc: 0.69969\n",
      "Epoch: 287 | Train Loss: 0.5027 | Train Acc: 0.7525 | Val Loss: 0.56977 | Val Acc: 0.69955\n",
      "Epoch: 288 | Train Loss: 0.5002 | Train Acc: 0.7554 | Val Loss: 0.57182 | Val Acc: 0.70251\n",
      "Epoch: 289 | Train Loss: 0.5008 | Train Acc: 0.7555 | Val Loss: 0.57021 | Val Acc: 0.69997\n",
      "Epoch: 290 | Train Loss: 0.5010 | Train Acc: 0.7555 | Val Loss: 0.57142 | Val Acc: 0.70054\n",
      "Epoch: 291 | Train Loss: 0.5014 | Train Acc: 0.7565 | Val Loss: 0.57133 | Val Acc: 0.69729\n",
      "Epoch: 292 | Train Loss: 0.5015 | Train Acc: 0.7545 | Val Loss: 0.57045 | Val Acc: 0.69715\n",
      "Epoch: 293 | Train Loss: 0.5001 | Train Acc: 0.7581 | Val Loss: 0.57111 | Val Acc: 0.70025\n",
      "Epoch: 294 | Train Loss: 0.4985 | Train Acc: 0.7563 | Val Loss: 0.57040 | Val Acc: 0.70068\n",
      "Epoch: 295 | Train Loss: 0.4991 | Train Acc: 0.7563 | Val Loss: 0.56966 | Val Acc: 0.70223\n",
      "Epoch: 296 | Train Loss: 0.5011 | Train Acc: 0.7564 | Val Loss: 0.57347 | Val Acc: 0.70251\n",
      "Epoch: 297 | Train Loss: 0.4995 | Train Acc: 0.7578 | Val Loss: 0.57532 | Val Acc: 0.70195\n",
      "Epoch: 298 | Train Loss: 0.4992 | Train Acc: 0.7560 | Val Loss: 0.57543 | Val Acc: 0.70096\n",
      "Epoch: 299 | Train Loss: 0.4985 | Train Acc: 0.7563 | Val Loss: 0.57996 | Val Acc: 0.70025\n",
      "Epoch: 300 | Train Loss: 0.4993 | Train Acc: 0.7562 | Val Loss: 0.57628 | Val Acc: 0.69842\n",
      "Epoch: 301 | Train Loss: 0.5001 | Train Acc: 0.7571 | Val Loss: 0.57786 | Val Acc: 0.69955\n",
      "Epoch: 302 | Train Loss: 0.5008 | Train Acc: 0.7583 | Val Loss: 0.57280 | Val Acc: 0.69884\n",
      "Epoch: 303 | Train Loss: 0.4973 | Train Acc: 0.7572 | Val Loss: 0.57153 | Val Acc: 0.69913\n",
      "Epoch: 304 | Train Loss: 0.4997 | Train Acc: 0.7556 | Val Loss: 0.57625 | Val Acc: 0.69941\n",
      "Epoch: 305 | Train Loss: 0.4986 | Train Acc: 0.7561 | Val Loss: 0.57304 | Val Acc: 0.69658\n",
      "Epoch: 306 | Train Loss: 0.4954 | Train Acc: 0.7590 | Val Loss: 0.57713 | Val Acc: 0.69785\n",
      "Epoch: 307 | Train Loss: 0.4963 | Train Acc: 0.7567 | Val Loss: 0.57530 | Val Acc: 0.69955\n",
      "Epoch: 308 | Train Loss: 0.4976 | Train Acc: 0.7574 | Val Loss: 0.57741 | Val Acc: 0.69856\n",
      "Epoch: 309 | Train Loss: 0.4977 | Train Acc: 0.7574 | Val Loss: 0.57806 | Val Acc: 0.69348\n",
      "Epoch: 310 | Train Loss: 0.4987 | Train Acc: 0.7587 | Val Loss: 0.57867 | Val Acc: 0.69673\n",
      "Epoch: 311 | Train Loss: 0.4977 | Train Acc: 0.7593 | Val Loss: 0.57205 | Val Acc: 0.69560\n",
      "Epoch: 312 | Train Loss: 0.4967 | Train Acc: 0.7584 | Val Loss: 0.57631 | Val Acc: 0.69658\n",
      "early stopping\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "\tn_students,\n",
    "\tn_questions,\n",
    "\ttrain_df,\n",
    "\tval_df,\n",
    "\tstudent_meta_tensor,\n",
    "\tstudent_embed_dim=8,\n",
    "\tquestion_embed_dim=16,\n",
    "\thidden_layers=[64, 16],\n",
    "\tdropout_p=0.3,\n",
    "\tbatch_size=32,\n",
    "\tlearning_rate=1e-4,\n",
    "\tdevice = \"cpu\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
