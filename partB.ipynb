{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train_data.csv\")\n",
    "val_df = pd.read_csv(\"data/valid_data.csv\")\n",
    "student_meta = pd.read_csv(\"data/student_meta.csv\")\n",
    "question_meta = pd.read_csv(\"data/question_meta.csv\")\n",
    "subject_meta = pd.read_csv(\"data/subject_meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542 1774\n"
     ]
    }
   ],
   "source": [
    "n_students = max(student_meta[\"user_id\"]) + 1\n",
    "n_questions = max(question_meta[\"question_id\"]) + 1\n",
    "print(n_students, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -1.],\n",
       "        [ 1., -1.],\n",
       "        [ 0., -1.],\n",
       "        ...,\n",
       "        [ 1., -1.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_meta_tensor = torch.zeros(n_students, 2)\n",
    "user_id = torch.tensor(student_meta['user_id'].values, dtype=torch.int32)\n",
    "gender = torch.tensor(student_meta['gender'].values, dtype=torch.float32)\n",
    "premium_pupil = torch.tensor(\n",
    "\tstudent_meta['premium_pupil'].fillna(-1.0).values, dtype=torch.float32\n",
    ")\n",
    "student_meta_tensor[user_id, 0] = gender\n",
    "student_meta_tensor[user_id, 1] = premium_pupil\n",
    "student_meta_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.user_ids = df['user_id'].values\n",
    "        self.question_ids = df['question_id'].values\n",
    "        self.is_correct = df['is_correct'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.is_correct)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.question_ids[idx], self.is_correct[idx]\n",
    "\n",
    "class StudentQuestionNet(nn.Module):\n",
    "    def __init__(self, student_embed_dim, question_embed_dim, student_meta_dim, hidden_layers, dropout_p=0.3):\n",
    "        super(StudentQuestionNet, self).__init__()\n",
    "        input_dim = student_embed_dim + question_embed_dim + student_meta_dim\n",
    "        layers = []\n",
    "\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_p))\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, student_embed, question_embed, student_meta):\n",
    "        combined = torch.cat([student_embed, question_embed, student_meta], dim=-1)\n",
    "        return self.network(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataloaders(train_df, val_df, batch_size):\n",
    "    train_dataset = QuestionDataset(train_df)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = QuestionDataset(val_df)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "def initialize_model_and_optimizers(n_students, n_questions, student_embed_dim, question_embed_dim, student_meta_dim,\n",
    "                                    hidden_layers, dropout_p, learning_rate, device):\n",
    "    student_embed = torch.nn.Parameter(torch.randn(n_students, student_embed_dim).to(device))\n",
    "    question_embed = torch.nn.Parameter(torch.randn(n_questions, question_embed_dim).to(device))\n",
    "\n",
    "    model = StudentQuestionNet(\n",
    "        student_embed_dim=student_embed_dim,\n",
    "        question_embed_dim=question_embed_dim,\n",
    "        student_meta_dim=student_meta_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        dropout_p=dropout_p\n",
    "    ).to(device)\n",
    "\n",
    "    model_optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    embed_optimizer = torch.optim.Adam([student_embed, question_embed], lr=learning_rate)\n",
    "\n",
    "    return model, student_embed, question_embed, model_optimizer, embed_optimizer\n",
    "\n",
    "def train_step(model, train_dataloader, student_embed, question_embed, student_meta_tensor, criterion, model_optimizer,\n",
    "               embed_optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for user_ids, question_ids, targets in train_dataloader:\n",
    "        user_ids = user_ids.to(device)\n",
    "        question_ids = question_ids.to(device)\n",
    "        targets = targets.float().unsqueeze(1).to(device)\n",
    "\n",
    "        user_embeds = student_embed[user_ids]\n",
    "        question_embeds = question_embed[question_ids]\n",
    "        student_meta = student_meta_tensor.to(device)[user_ids]\n",
    "\n",
    "        logits = model(user_embeds, question_embeds, student_meta)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        model_optimizer.zero_grad()\n",
    "        embed_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "        embed_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predictions = torch.sigmoid(logits) > 0.5\n",
    "        correct_predictions += (predictions == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "    return train_loss / len(train_dataloader), correct_predictions / total_samples\n",
    "\n",
    "\n",
    "def val_step(model, val_dataloader, student_embed, question_embed, student_meta_tensor, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user_ids, question_ids, targets in val_dataloader:\n",
    "            user_ids = user_ids.to(device)\n",
    "            question_ids = question_ids.to(device)\n",
    "            targets = targets.float().unsqueeze(1).to(device)\n",
    "\n",
    "            user_embeds = student_embed[user_ids]\n",
    "            question_embeds = question_embed[question_ids]\n",
    "            student_meta = student_meta_tensor.to(device)[user_ids]\n",
    "\n",
    "            logits = model(user_embeds, question_embeds, student_meta)\n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predictions = torch.sigmoid(logits) > 0.5\n",
    "            correct_predictions += (predictions == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    return val_loss / len(val_dataloader), correct_predictions / total_samples\n",
    "\n",
    "def checkpoint(epoch, experiment_path, avg_val_loss, val_accuracy, model, student_embed, question_embed,\n",
    "                     best_val_acc):\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "\n",
    "        for file in os.listdir(experiment_path):\n",
    "            if file.endswith('.pt'):\n",
    "                os.remove(os.path.join(experiment_path, file))\n",
    "\n",
    "        model_filename = f'epoch{epoch}_val_loss{avg_val_loss:.5f}_val_acc{val_accuracy:.5f}.pt'\n",
    "        model_save_path = os.path.join(experiment_path, model_filename)\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "        torch.save(student_embed, os.path.join(experiment_path, 'student_embed.pt'))\n",
    "        torch.save(question_embed, os.path.join(experiment_path, 'question_embed.pt'))\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "def train_model(\n",
    "    n_students,\n",
    "    n_questions,\n",
    "    train_df,\n",
    "    val_df,\n",
    "    student_meta_tensor,\n",
    "    student_embed_dim=8,\n",
    "    question_embed_dim=16,\n",
    "    hidden_layers=[64, 16],\n",
    "    dropout_p=0.3,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-3,\n",
    "    device='cpu'\n",
    "):\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    log_dir = 'logs'\n",
    "\n",
    "    experiment_num = 1\n",
    "    while os.path.exists(os.path.join(checkpoint_dir, f'experiment{experiment_num}')):\n",
    "        experiment_num += 1\n",
    "    experiment_path = os.path.join(checkpoint_dir, f'experiment{experiment_num}')\n",
    "    os.makedirs(experiment_path, exist_ok=True)\n",
    "\n",
    "    hyperparameters = {\n",
    "        'n_students': n_students,\n",
    "        'n_questions': n_questions,\n",
    "        'student_embed_dim': student_embed_dim,\n",
    "        'question_embed_dim': question_embed_dim,\n",
    "        'hidden_layers': hidden_layers,\n",
    "        'dropout_p': dropout_p,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'device': device\n",
    "    }\n",
    "    with open(os.path.join(experiment_path, 'hyperparameters.json'), 'w') as f:\n",
    "        json.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "    writer = SummaryWriter(os.path.join(log_dir, f'experiment{experiment_num}'))\n",
    "    train_dataloader, val_dataloader = initialize_dataloaders(train_df, val_df, batch_size)\n",
    "\n",
    "    model, student_embed, question_embed, model_optimizer, embed_optimizer = initialize_model_and_optimizers(\n",
    "        n_students, n_questions, student_embed_dim, question_embed_dim, student_meta_tensor.shape[1],\n",
    "        hidden_layers, dropout_p, learning_rate, device\n",
    "    )\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    best_val_acc = -1\n",
    "    epoch = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            epoch += 1\n",
    "\n",
    "            avg_train_loss, train_accuracy = train_step(\n",
    "                model, train_dataloader, student_embed, question_embed, student_meta_tensor,\n",
    "                criterion, model_optimizer, embed_optimizer, device\n",
    "            )\n",
    "\n",
    "            avg_val_loss, val_accuracy = val_step(\n",
    "                model, val_dataloader, student_embed, question_embed, student_meta_tensor,\n",
    "                criterion, device\n",
    "            )\n",
    "\n",
    "            writer.add_scalar('Train/Loss', avg_train_loss, epoch)\n",
    "            writer.add_scalar('Train/Accuracy', train_accuracy, epoch)\n",
    "            writer.add_scalar('Validation/Loss', avg_val_loss, epoch)\n",
    "            writer.add_scalar('Validation/Accuracy', val_accuracy, epoch)\n",
    "\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.5f} | Train Acc: {train_accuracy:.4f} | Val Acc: {val_accuracy:.5f}\")\n",
    "\n",
    "            best_val_acc = checkpoint(\n",
    "                epoch, experiment_path, avg_val_loss, val_accuracy, model, student_embed, question_embed, best_val_acc\n",
    "            )\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"early stopping\")\n",
    "    finally:\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    n_students,\n",
    "    n_questions,\n",
    "    train_df,\n",
    "    val_df,\n",
    "    student_meta_tensor,\n",
    "    student_embed_dim=16,\n",
    "    question_embed_dim=64,\n",
    "    hidden_layers=[16, 128, 16],\n",
    "    dropout_p=0.5,\n",
    "    batch_size=256,\n",
    "    learning_rate=1e-4,\n",
    "\tdevice = \"mps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
